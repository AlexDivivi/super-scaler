{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(83750, 1800, 3)\n",
      "(80000, 1800, 3)\n",
      "(86350, 1800, 3)\n",
      "(39700, 1800, 3)\n",
      "(68600, 1800, 3)\n",
      "(65150, 1800, 3)\n",
      "(68100, 1800, 3)\n"
     ]
    }
   ],
   "source": [
    "all_data = []\n",
    "image_chop = 50\n",
    "\n",
    "for i in range(0,7):\n",
    "    pkl_file = open(f'../data/cat{i}.pkl', 'rb')\n",
    "    data = pickle.load(pkl_file)\n",
    "    data = data.reshape(int(data.shape[0] * image_chop), int(data.shape[1] / image_chop), 3)\n",
    "    print(data.shape)\n",
    "    pkl_file.close()\n",
    "    all_data.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "(83750, 1800, 3)\n",
      "(80000, 1800, 3)\n",
      "(86350, 1800, 3)\n",
      "(39700, 1800, 3)\n",
      "(68600, 1800, 3)\n",
      "(65150, 1800, 3)\n",
      "(68100, 1800, 3)\n",
      "Sample size: 491650\n"
     ]
    }
   ],
   "source": [
    "print(len(all_data))\n",
    "samples = 0\n",
    "for i in range(0,7):\n",
    "    print(all_data[i].shape)\n",
    "    samples += all_data[i].shape[0]\n",
    "print(f'Sample size: {samples}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "D_in = all_data[1].shape[1]\n",
    "H1 = int(all_data[1].shape[1]*0.75)\n",
    "D_out = int(all_data[1].shape[1] / 2)\n",
    "\n",
    "learning_rate = 0.0001\n",
    "batch_size = image_chop * 10\n",
    "epochs = 1000\n",
    "\n",
    "# Regularisierung\n",
    "weight_decay=0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network\n",
    "class autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(autoencoder, self).__init__()\n",
    "        \n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            nn.Linear(D_in, H1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(H1, D_out)\n",
    "        )\n",
    "\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            nn.Linear(D_out, H1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(H1, D_in),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "autoencoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=1800, out_features=1350, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=1350, out_features=900, bias=True)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=900, out_features=1350, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=1350, out_features=1800, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = autoencoder()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train loss: 0.0561690516769886\n",
      "2 Train loss: 0.05594406649470329\n",
      "4 Train loss: 0.05597377195954323\n",
      "6 Train loss: 0.0559353344142437\n",
      "8 Train loss: 0.05590115115046501\n",
      "10 Train loss: 0.055870745331048965\n",
      "12 Train loss: 0.05584243685007095\n",
      "14 Train loss: 0.055809520184993744\n",
      "16 Train loss: 0.05577174946665764\n",
      "18 Train loss: 0.05574220046401024\n",
      "20 Train loss: 0.055721789598464966\n",
      "22 Train loss: 0.05570690333843231\n",
      "24 Train loss: 0.05569472163915634\n",
      "26 Train loss: 0.05568545311689377\n",
      "28 Train loss: 0.05567675828933716\n",
      "30 Train loss: 0.055668704211711884\n",
      "32 Train loss: 0.05566300451755524\n",
      "34 Train loss: 0.05565869063138962\n",
      "36 Train loss: 0.05565539747476578\n",
      "38 Train loss: 0.05565216392278671\n",
      "40 Train loss: 0.05564982071518898\n",
      "42 Train loss: 0.05564814805984497\n",
      "44 Train loss: 0.055646661669015884\n",
      "46 Train loss: 0.05564500764012337\n",
      "48 Train loss: 0.05564340204000473\n",
      "50 Train loss: 0.055642176419496536\n",
      "52 Train loss: 0.0556403249502182\n",
      "54 Train loss: 0.055637892335653305\n",
      "56 Train loss: 0.0556350015103817\n",
      "58 Train loss: 0.055632591247558594\n",
      "60 Train loss: 0.055630411952733994\n",
      "62 Train loss: 0.055628519505262375\n",
      "64 Train loss: 0.055626511573791504\n",
      "66 Train loss: 0.05562492460012436\n",
      "68 Train loss: 0.05562334507703781\n",
      "70 Train loss: 0.05562138929963112\n",
      "72 Train loss: 0.05561993643641472\n",
      "74 Train loss: 0.05561893433332443\n",
      "76 Train loss: 0.05561762675642967\n",
      "78 Train loss: 0.055616360157728195\n",
      "80 Train loss: 0.05561460554599762\n",
      "82 Train loss: 0.055613528937101364\n",
      "84 Train loss: 0.055612049996852875\n",
      "86 Train loss: 0.05561087280511856\n",
      "88 Train loss: 0.05560961738228798\n",
      "90 Train loss: 0.05560951307415962\n",
      "92 Train loss: 0.05560927093029022\n",
      "94 Train loss: 0.05560893565416336\n",
      "96 Train loss: 0.05560855194926262\n",
      "98 Train loss: 0.0556081086397171\n",
      "100 Train loss: 0.0556073859333992\n",
      "102 Train loss: 0.055606696754693985\n",
      "104 Train loss: 0.05560589209198952\n",
      "106 Train loss: 0.05560476332902908\n",
      "108 Train loss: 0.05560419708490372\n",
      "110 Train loss: 0.05560343712568283\n",
      "112 Train loss: 0.0556027777493\n",
      "114 Train loss: 0.05560208484530449\n",
      "116 Train loss: 0.05560070648789406\n",
      "118 Train loss: 0.05559961870312691\n",
      "120 Train loss: 0.05559953674674034\n",
      "122 Train loss: 0.05559924617409706\n",
      "124 Train loss: 0.05559917911887169\n",
      "126 Train loss: 0.05559873953461647\n",
      "128 Train loss: 0.05559827387332916\n",
      "130 Train loss: 0.055597707629203796\n",
      "132 Train loss: 0.05559723824262619\n",
      "134 Train loss: 0.0555967316031456\n",
      "136 Train loss: 0.05559573322534561\n",
      "138 Train loss: 0.05559476837515831\n",
      "140 Train loss: 0.055593714118003845\n",
      "142 Train loss: 0.05559343844652176\n",
      "144 Train loss: 0.055593427270650864\n",
      "146 Train loss: 0.055593568831682205\n",
      "148 Train loss: 0.05559346079826355\n",
      "150 Train loss: 0.055593181401491165\n",
      "152 Train loss: 0.05559328570961952\n",
      "154 Train loss: 0.055592916905879974\n",
      "156 Train loss: 0.05559288710355759\n",
      "158 Train loss: 0.05559298023581505\n",
      "160 Train loss: 0.05559258535504341\n",
      "162 Train loss: 0.05559265613555908\n",
      "164 Train loss: 0.05559224635362625\n",
      "166 Train loss: 0.055592235177755356\n",
      "168 Train loss: 0.05559215322136879\n",
      "170 Train loss: 0.05559214577078819\n",
      "172 Train loss: 0.05559181049466133\n",
      "174 Train loss: 0.055591560900211334\n",
      "176 Train loss: 0.055591482669115067\n",
      "178 Train loss: 0.05559126287698746\n",
      "180 Train loss: 0.055591121315956116\n",
      "182 Train loss: 0.055590759962797165\n",
      "184 Train loss: 0.05559067055583\n",
      "186 Train loss: 0.05559055507183075\n",
      "188 Train loss: 0.05559007450938225\n",
      "190 Train loss: 0.05558975040912628\n",
      "192 Train loss: 0.05558926984667778\n",
      "194 Train loss: 0.05558909848332405\n",
      "196 Train loss: 0.055588629096746445\n",
      "198 Train loss: 0.055588193237781525\n",
      "200 Train loss: 0.055587783455848694\n",
      "202 Train loss: 0.05558709055185318\n",
      "204 Train loss: 0.05558634176850319\n",
      "206 Train loss: 0.0555860660970211\n",
      "208 Train loss: 0.05558578297495842\n",
      "210 Train loss: 0.055585429072380066\n",
      "212 Train loss: 0.05558496713638306\n",
      "214 Train loss: 0.05558383837342262\n",
      "216 Train loss: 0.05558311194181442\n",
      "218 Train loss: 0.055582787841558456\n",
      "220 Train loss: 0.055582720786333084\n",
      "222 Train loss: 0.055582743138074875\n",
      "224 Train loss: 0.055582690984010696\n",
      "226 Train loss: 0.05558263137936592\n",
      "228 Train loss: 0.055582962930202484\n",
      "230 Train loss: 0.055582787841558456\n",
      "232 Train loss: 0.05558335781097412\n",
      "234 Train loss: 0.05558262765407562\n",
      "236 Train loss: 0.05558260902762413\n",
      "238 Train loss: 0.0555826798081398\n",
      "240 Train loss: 0.055582545697689056\n",
      "242 Train loss: 0.055582720786333084\n",
      "244 Train loss: 0.05558276176452637\n",
      "246 Train loss: 0.055582500994205475\n",
      "248 Train loss: 0.05558269843459129\n",
      "250 Train loss: 0.055582575500011444\n",
      "252 Train loss: 0.055582452565431595\n",
      "254 Train loss: 0.05558265745639801\n",
      "256 Train loss: 0.05558260902762413\n",
      "258 Train loss: 0.05558232590556145\n",
      "260 Train loss: 0.05558241158723831\n",
      "262 Train loss: 0.05558253079652786\n",
      "264 Train loss: 0.05558247119188309\n",
      "266 Train loss: 0.05558251217007637\n",
      "268 Train loss: 0.05558248981833458\n",
      "270 Train loss: 0.055582478642463684\n",
      "272 Train loss: 0.05558238923549652\n",
      "274 Train loss: 0.05558248981833458\n",
      "276 Train loss: 0.055582307279109955\n",
      "278 Train loss: 0.055582448840141296\n",
      "280 Train loss: 0.05558235943317413\n",
      "282 Train loss: 0.05558247119188309\n",
      "284 Train loss: 0.0555821992456913\n",
      "286 Train loss: 0.05558238551020622\n",
      "288 Train loss: 0.055582430213689804\n",
      "290 Train loss: 0.0555821992456913\n",
      "292 Train loss: 0.05558232218027115\n",
      "294 Train loss: 0.05558257922530174\n",
      "296 Train loss: 0.05558260902762413\n",
      "298 Train loss: 0.05558241903781891\n",
      "300 Train loss: 0.055582206696271896\n",
      "302 Train loss: 0.05558229237794876\n",
      "304 Train loss: 0.05558212846517563\n",
      "306 Train loss: 0.055582281202077866\n",
      "308 Train loss: 0.055582139641046524\n",
      "310 Train loss: 0.05558197200298309\n",
      "312 Train loss: 0.055582255125045776\n",
      "314 Train loss: 0.05558202043175697\n",
      "316 Train loss: 0.05558275431394577\n",
      "318 Train loss: 0.055582355707883835\n",
      "320 Train loss: 0.05558214709162712\n",
      "322 Train loss: 0.05558200925588608\n",
      "324 Train loss: 0.055582284927368164\n",
      "326 Train loss: 0.055582303553819656\n",
      "328 Train loss: 0.05558193102478981\n",
      "330 Train loss: 0.05558207258582115\n",
      "332 Train loss: 0.05558203533291817\n",
      "334 Train loss: 0.05558200180530548\n",
      "336 Train loss: 0.05558187514543533\n",
      "338 Train loss: 0.055582061409950256\n",
      "340 Train loss: 0.05558181181550026\n",
      "342 Train loss: 0.05558200925588608\n",
      "344 Train loss: 0.05558193102478981\n",
      "346 Train loss: 0.055581770837306976\n",
      "348 Train loss: 0.05558208376169205\n",
      "350 Train loss: 0.05558193102478981\n",
      "352 Train loss: 0.05558188632130623\n",
      "354 Train loss: 0.0555819571018219\n",
      "356 Train loss: 0.0555819496512413\n",
      "358 Train loss: 0.05558161437511444\n",
      "360 Train loss: 0.05558173730969429\n",
      "362 Train loss: 0.055581752210855484\n",
      "364 Train loss: 0.05558175966143608\n",
      "366 Train loss: 0.055581770837306976\n",
      "368 Train loss: 0.05558159202337265\n",
      "370 Train loss: 0.05558179318904877\n",
      "372 Train loss: 0.05558155104517937\n",
      "374 Train loss: 0.05558133125305176\n",
      "376 Train loss: 0.05558145418763161\n",
      "378 Train loss: 0.05558155104517937\n",
      "380 Train loss: 0.05558144673705101\n",
      "382 Train loss: 0.05558141693472862\n",
      "384 Train loss: 0.05558134987950325\n",
      "386 Train loss: 0.05558149889111519\n",
      "388 Train loss: 0.055581312626600266\n",
      "390 Train loss: 0.05558174103498459\n",
      "392 Train loss: 0.055581334978342056\n",
      "394 Train loss: 0.055581219494342804\n",
      "396 Train loss: 0.05558139458298683\n",
      "398 Train loss: 0.055581219494342804\n",
      "400 Train loss: 0.055581219494342804\n",
      "402 Train loss: 0.05558127164840698\n",
      "404 Train loss: 0.055581122636795044\n",
      "406 Train loss: 0.05558095499873161\n",
      "408 Train loss: 0.05558103695511818\n",
      "410 Train loss: 0.055580999702215195\n",
      "412 Train loss: 0.05558091029524803\n",
      "414 Train loss: 0.05558084324002266\n",
      "416 Train loss: 0.05558096617460251\n",
      "418 Train loss: 0.055580612272024155\n",
      "420 Train loss: 0.05558076500892639\n",
      "422 Train loss: 0.055580705404281616\n",
      "424 Train loss: 0.055580880492925644\n",
      "426 Train loss: 0.05558067560195923\n",
      "428 Train loss: 0.05558045208454132\n",
      "430 Train loss: 0.05558048188686371\n",
      "432 Train loss: 0.0555805079638958\n",
      "434 Train loss: 0.05558021366596222\n",
      "436 Train loss: 0.05558033660054207\n",
      "438 Train loss: 0.05558045580983162\n",
      "440 Train loss: 0.055580075830221176\n",
      "442 Train loss: 0.055580079555511475\n",
      "444 Train loss: 0.05558018386363983\n",
      "446 Train loss: 0.055579908192157745\n",
      "448 Train loss: 0.055579833686351776\n",
      "450 Train loss: 0.05557984858751297\n",
      "452 Train loss: 0.05557965114712715\n",
      "454 Train loss: 0.05557968094944954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "456 Train loss: 0.05557950213551521\n",
      "458 Train loss: 0.05557940527796745\n",
      "460 Train loss: 0.05557934194803238\n",
      "462 Train loss: 0.05557901784777641\n",
      "464 Train loss: 0.055579084903001785\n",
      "466 Train loss: 0.05557894706726074\n",
      "468 Train loss: 0.05557872727513313\n",
      "470 Train loss: 0.055578604340553284\n",
      "472 Train loss: 0.055578552186489105\n",
      "474 Train loss: 0.055578406900167465\n",
      "476 Train loss: 0.055578142404556274\n",
      "478 Train loss: 0.05557797849178314\n",
      "480 Train loss: 0.055577777326107025\n",
      "482 Train loss: 0.0555775985121727\n",
      "484 Train loss: 0.05557756870985031\n",
      "486 Train loss: 0.05557743459939957\n",
      "488 Train loss: 0.05557723343372345\n",
      "490 Train loss: 0.05557680130004883\n",
      "492 Train loss: 0.05557652935385704\n",
      "494 Train loss: 0.055576346814632416\n",
      "496 Train loss: 0.055575892329216\n",
      "498 Train loss: 0.05557554215192795\n",
      "500 Train loss: 0.05557519197463989\n",
      "502 Train loss: 0.05557458847761154\n",
      "504 Train loss: 0.05557425320148468\n",
      "506 Train loss: 0.055574700236320496\n",
      "508 Train loss: 0.055574480444192886\n",
      "510 Train loss: 0.05557437613606453\n",
      "512 Train loss: 0.0555744543671608\n",
      "514 Train loss: 0.05557449534535408\n",
      "516 Train loss: 0.055574264377355576\n",
      "518 Train loss: 0.05557439103722572\n",
      "520 Train loss: 0.055574480444192886\n",
      "522 Train loss: 0.05557436868548393\n",
      "524 Train loss: 0.05557471141219139\n",
      "526 Train loss: 0.0555744394659996\n",
      "528 Train loss: 0.055574432015419006\n",
      "530 Train loss: 0.0555744543671608\n",
      "532 Train loss: 0.055574484169483185\n",
      "534 Train loss: 0.05557435750961304\n",
      "536 Train loss: 0.05557439103722572\n",
      "538 Train loss: 0.055574189871549606\n",
      "540 Train loss: 0.055574409663677216\n",
      "542 Train loss: 0.05557447299361229\n",
      "544 Train loss: 0.055574506521224976\n",
      "546 Train loss: 0.05557441711425781\n",
      "548 Train loss: 0.05557457357645035\n",
      "550 Train loss: 0.05557452514767647\n",
      "552 Train loss: 0.05557435005903244\n",
      "554 Train loss: 0.05557452514767647\n",
      "556 Train loss: 0.05557435750961304\n",
      "558 Train loss: 0.055574558675289154\n",
      "560 Train loss: 0.0555742010474205\n",
      "562 Train loss: 0.055574338883161545\n",
      "564 Train loss: 0.05557454377412796\n",
      "566 Train loss: 0.055574484169483185\n",
      "568 Train loss: 0.05557439103722572\n",
      "570 Train loss: 0.05557458847761154\n",
      "572 Train loss: 0.05557428300380707\n",
      "574 Train loss: 0.055574387311935425\n",
      "576 Train loss: 0.05557454749941826\n",
      "578 Train loss: 0.055574335157871246\n",
      "580 Train loss: 0.055574532598257065\n",
      "582 Train loss: 0.05557437613606453\n",
      "584 Train loss: 0.05557423084974289\n",
      "586 Train loss: 0.05557423084974289\n",
      "588 Train loss: 0.05557423084974289\n",
      "590 Train loss: 0.05557432398200035\n",
      "592 Train loss: 0.0555742122232914\n",
      "594 Train loss: 0.05557458847761154\n",
      "596 Train loss: 0.0555742010474205\n",
      "598 Train loss: 0.05557447299361229\n",
      "600 Train loss: 0.05557423457503319\n",
      "602 Train loss: 0.05557429417967796\n",
      "604 Train loss: 0.055574219673871994\n",
      "606 Train loss: 0.05557452514767647\n",
      "608 Train loss: 0.0555741973221302\n",
      "610 Train loss: 0.05557417869567871\n",
      "612 Train loss: 0.05557427555322647\n",
      "614 Train loss: 0.05557442083954811\n",
      "616 Train loss: 0.05557426065206528\n",
      "618 Train loss: 0.0555744431912899\n",
      "620 Train loss: 0.055574286729097366\n",
      "622 Train loss: 0.05557427555322647\n",
      "624 Train loss: 0.05557442083954811\n",
      "626 Train loss: 0.05557424947619438\n",
      "628 Train loss: 0.055574286729097366\n",
      "630 Train loss: 0.055574219673871994\n",
      "632 Train loss: 0.05557424947619438\n",
      "634 Train loss: 0.055574171245098114\n",
      "636 Train loss: 0.055574242025613785\n",
      "638 Train loss: 0.05557426065206528\n",
      "640 Train loss: 0.05557406693696976\n",
      "642 Train loss: 0.05557417869567871\n",
      "644 Train loss: 0.05557416006922722\n",
      "646 Train loss: 0.05557428300380707\n",
      "648 Train loss: 0.055574167519807816\n",
      "650 Train loss: 0.0555742122232914\n",
      "652 Train loss: 0.055574145168066025\n",
      "654 Train loss: 0.05557412654161453\n",
      "656 Train loss: 0.05557414889335632\n",
      "658 Train loss: 0.055574096739292145\n",
      "660 Train loss: 0.05557428300380707\n",
      "662 Train loss: 0.055574093014001846\n",
      "664 Train loss: 0.05557410791516304\n",
      "666 Train loss: 0.05557405576109886\n",
      "668 Train loss: 0.055574167519807816\n",
      "670 Train loss: 0.05557429417967796\n",
      "672 Train loss: 0.055574167519807816\n",
      "674 Train loss: 0.055574093014001846\n",
      "676 Train loss: 0.05557425320148468\n",
      "678 Train loss: 0.055574264377355576\n",
      "680 Train loss: 0.05557435005903244\n",
      "682 Train loss: 0.055574242025613785\n",
      "684 Train loss: 0.055574025958776474\n",
      "686 Train loss: 0.055574022233486176\n",
      "688 Train loss: 0.05557415634393692\n",
      "690 Train loss: 0.055573999881744385\n",
      "692 Train loss: 0.055574074387550354\n",
      "694 Train loss: 0.05557426065206528\n",
      "696 Train loss: 0.05557423457503319\n",
      "698 Train loss: 0.055573977530002594\n",
      "700 Train loss: 0.055573947727680206\n",
      "702 Train loss: 0.055574093014001846\n",
      "704 Train loss: 0.05557428300380707\n",
      "706 Train loss: 0.05557398125529289\n",
      "708 Train loss: 0.05557435750961304\n",
      "710 Train loss: 0.05557405576109886\n",
      "712 Train loss: 0.0555742010474205\n",
      "714 Train loss: 0.05557427182793617\n",
      "716 Train loss: 0.05557399243116379\n",
      "718 Train loss: 0.05557415634393692\n",
      "720 Train loss: 0.055573999881744385\n",
      "722 Train loss: 0.05557405203580856\n",
      "724 Train loss: 0.055574025958776474\n",
      "726 Train loss: 0.05557405203580856\n",
      "728 Train loss: 0.055573999881744385\n",
      "730 Train loss: 0.05557416006922722\n",
      "732 Train loss: 0.055573947727680206\n",
      "734 Train loss: 0.055573929101228714\n",
      "736 Train loss: 0.05557403340935707\n",
      "738 Train loss: 0.055574093014001846\n",
      "740 Train loss: 0.055573999881744385\n",
      "742 Train loss: 0.055573999881744385\n",
      "744 Train loss: 0.055574264377355576\n",
      "746 Train loss: 0.055574096739292145\n",
      "748 Train loss: 0.05557401478290558\n",
      "750 Train loss: 0.055574096739292145\n",
      "752 Train loss: 0.055573977530002594\n",
      "754 Train loss: 0.05557413771748543\n",
      "756 Train loss: 0.055574096739292145\n",
      "758 Train loss: 0.05557389557361603\n",
      "760 Train loss: 0.05557390674948692\n",
      "762 Train loss: 0.05557405576109886\n",
      "764 Train loss: 0.05557404085993767\n",
      "766 Train loss: 0.05557400360703468\n",
      "768 Train loss: 0.05557404085993767\n",
      "770 Train loss: 0.055574189871549606\n",
      "772 Train loss: 0.0555742010474205\n",
      "774 Train loss: 0.05557388812303543\n",
      "776 Train loss: 0.05557401105761528\n",
      "778 Train loss: 0.05557390674948692\n",
      "780 Train loss: 0.05557386577129364\n",
      "782 Train loss: 0.05557404085993767\n",
      "784 Train loss: 0.05557393655180931\n",
      "786 Train loss: 0.05557389557361603\n",
      "788 Train loss: 0.05557398870587349\n",
      "790 Train loss: 0.05557399243116379\n",
      "792 Train loss: 0.05557388439774513\n",
      "794 Train loss: 0.055573854595422745\n",
      "796 Train loss: 0.055573832243680954\n",
      "798 Train loss: 0.0555739589035511\n",
      "800 Train loss: 0.055573925375938416\n",
      "802 Train loss: 0.055573947727680206\n",
      "804 Train loss: 0.055574022233486176\n",
      "806 Train loss: 0.05557398125529289\n",
      "808 Train loss: 0.055573951452970505\n",
      "810 Train loss: 0.055573754012584686\n",
      "812 Train loss: 0.05557394027709961\n",
      "814 Train loss: 0.05557413026690483\n",
      "816 Train loss: 0.05557383596897125\n",
      "818 Train loss: 0.05557398125529289\n",
      "820 Train loss: 0.055573977530002594\n",
      "822 Train loss: 0.055573806166648865\n",
      "824 Train loss: 0.05557384341955185\n",
      "826 Train loss: 0.05557381361722946\n",
      "828 Train loss: 0.05557386577129364\n",
      "830 Train loss: 0.05557391047477722\n",
      "832 Train loss: 0.05557386577129364\n",
      "834 Train loss: 0.05557373911142349\n",
      "836 Train loss: 0.05557379499077797\n",
      "838 Train loss: 0.05557406321167946\n",
      "840 Train loss: 0.055573899298906326\n",
      "842 Train loss: 0.05557377263903618\n",
      "844 Train loss: 0.05557379499077797\n",
      "846 Train loss: 0.05557390674948692\n",
      "848 Train loss: 0.055573802441358566\n",
      "850 Train loss: 0.05557386577129364\n",
      "852 Train loss: 0.055573754012584686\n",
      "854 Train loss: 0.05557381361722946\n",
      "856 Train loss: 0.05557364597916603\n",
      "858 Train loss: 0.05557391047477722\n",
      "860 Train loss: 0.05557383596897125\n",
      "862 Train loss: 0.05557398870587349\n",
      "864 Train loss: 0.05557360500097275\n",
      "866 Train loss: 0.05557376146316528\n",
      "868 Train loss: 0.05557376891374588\n",
      "870 Train loss: 0.05557364597916603\n",
      "872 Train loss: 0.05557376891374588\n",
      "874 Train loss: 0.055573802441358566\n",
      "876 Train loss: 0.055574074387550354\n",
      "878 Train loss: 0.05557393655180931\n",
      "880 Train loss: 0.05557362735271454\n",
      "882 Train loss: 0.05557359382510185\n",
      "884 Train loss: 0.055573731660842896\n",
      "886 Train loss: 0.055573783814907074\n",
      "888 Train loss: 0.05557373911142349\n",
      "890 Train loss: 0.0555737167596817\n",
      "892 Train loss: 0.05557367578148842\n",
      "894 Train loss: 0.05557376146316528\n",
      "896 Train loss: 0.05557370185852051\n",
      "898 Train loss: 0.055573709309101105\n",
      "900 Train loss: 0.055573686957359314\n",
      "902 Train loss: 0.0555737167596817\n",
      "904 Train loss: 0.055573638528585434\n",
      "906 Train loss: 0.055573999881744385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "908 Train loss: 0.05557377263903618\n",
      "910 Train loss: 0.0555737279355526\n",
      "912 Train loss: 0.055573634803295135\n",
      "914 Train loss: 0.055574025958776474\n",
      "916 Train loss: 0.05557384341955185\n",
      "918 Train loss: 0.05557360127568245\n",
      "920 Train loss: 0.05557383596897125\n",
      "922 Train loss: 0.055573612451553345\n",
      "924 Train loss: 0.0555737279355526\n",
      "926 Train loss: 0.05557362362742424\n",
      "928 Train loss: 0.055573634803295135\n",
      "930 Train loss: 0.05557357519865036\n",
      "932 Train loss: 0.055573832243680954\n",
      "934 Train loss: 0.055573638528585434\n",
      "936 Train loss: 0.05557383596897125\n",
      "938 Train loss: 0.05557360127568245\n",
      "940 Train loss: 0.0555734820663929\n",
      "942 Train loss: 0.05557366833090782\n",
      "944 Train loss: 0.05557365342974663\n",
      "946 Train loss: 0.05557354912161827\n",
      "948 Train loss: 0.055573783814907074\n",
      "950 Train loss: 0.05557360500097275\n",
      "952 Train loss: 0.055573560297489166\n",
      "954 Train loss: 0.05557384341955185\n",
      "956 Train loss: 0.05557362735271454\n",
      "958 Train loss: 0.0555737167596817\n",
      "960 Train loss: 0.05557365342974663\n",
      "962 Train loss: 0.055573444813489914\n",
      "964 Train loss: 0.05557384341955185\n",
      "966 Train loss: 0.055573541671037674\n",
      "968 Train loss: 0.05557367578148842\n",
      "970 Train loss: 0.05557353049516678\n",
      "972 Train loss: 0.05557342991232872\n",
      "974 Train loss: 0.05557354912161827\n",
      "976 Train loss: 0.05557359382510185\n",
      "978 Train loss: 0.05557349696755409\n",
      "980 Train loss: 0.05557353049516678\n",
      "982 Train loss: 0.05557338148355484\n",
      "984 Train loss: 0.05557349696755409\n",
      "986 Train loss: 0.05557351931929588\n",
      "988 Train loss: 0.05557351931929588\n",
      "990 Train loss: 0.05557345971465111\n",
      "992 Train loss: 0.05557350069284439\n",
      "994 Train loss: 0.05557357147336006\n",
      "996 Train loss: 0.055573780089616776\n",
      "998 Train loss: 0.05557353049516678\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "loss_hist = []\n",
    "\n",
    "for t in range(epochs):\n",
    "    for bulk in range(len(all_data)):\n",
    "        x = torch.tensor(all_data[bulk], dtype=torch.float32, device=device)\n",
    "        for batch in range(0, int(x.shape[0]/batch_size)):\n",
    "            \n",
    "            # Berechne den Batch\n",
    "            batch_x = x[batch * batch_size : (batch + 1) * batch_size, :].transpose(1, 2)\n",
    "        \n",
    "            # Berechne die Vorhersage (foward step)\n",
    "            outputs = model.forward(batch_x)\n",
    "\n",
    "            # Berechne den Fehler\n",
    "            loss = criterion(outputs, batch_x)\n",
    "        \n",
    "            # Berechne die Gradienten und Aktualisiere die Gewichte (backward step)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "    # Berechne den Fehler (Ausgabe des Fehlers alle x Iterationen)\n",
    "    if t % 2 == 0:\n",
    "        loss_hist.append(loss.item())\n",
    "        print(t, f\"Train loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8122d9d048>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/.local/lib/python3.7/site-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type autoencoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.cpu(), '../data/models/ae_x2_sigmoid.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
